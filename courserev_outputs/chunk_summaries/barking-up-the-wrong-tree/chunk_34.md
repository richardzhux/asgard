# Chunk 34 (units 27061-27960)

- **Key Concepts / Sections**
  - Dual-process thinking: *System 1* vs *System 2*
  - Heuristics in decision-making (shortcuts)
  - Question substitution (changing a hard question into an easier one)
  - Availability heuristic (main focus here)
  - Affect heuristic (named, detailed elsewhere)
  - Risk perception errors (plane crashes, Ebola, etc.)

- **Definitions**
  - **System 1**: Fast, automatic, intuitive thinking. Handles simple, routine actions (e.g., raising your right hand) and uses heuristics.  
  - **System 2**: Slow, effortful, deliberative thinking. Used for complex tasks (e.g., computing 1157 ÷ 13).
  - **Heuristics**: Mental shortcuts that allow quick judgments without full logical analysis; often helpful but can systematically mislead.
  - **Question substitution**: When faced with a complex question, the mind unconsciously replaces it with a simpler one it can answer, without noticing the swap.
    - Complex: “How risky is X, objectively?”  
    - Substituted: “How easily can I recall examples of X?”
  - **Availability heuristic**: Judging probability, frequency, or risk based on how easily examples come to mind. What’s more *available* in memory feels more likely/common/dangerous.
  - **Affect heuristic**: Judging risk or benefit based on emotional reactions (e.g., fear, liking/disliking) rather than analysis (referenced but not yet explained in detail here).

- **How the Availability Heuristic Works**
  - To estimate:
    - How likely something is
    - How common it is
    - How risky it is  
  - People:
    1. Recall examples.
    2. Treat ease/vividness of recall as a cue for probability or risk.
  - Events more likely to be recalled:
    - Recent
    - Vividly presented
    - Emotionally engaging  
  → These get judged as more probable or more dangerous than they really are.

- **Examples**
  - **Plane crashes vs car accidents**
    - Plane crashes are rare but:
      - Receive intense, vivid media coverage.
      - Are emotionally dramatic and memorable.
    → People overestimate the risk of flying relative to driving.
    - After 9/11:
      - Many Americans switched from planes to cars.
      - A study (Gigerenzer) found ~1,595 additional road deaths in that year due to increased driving.
      - When flying/driving returned to normal, highway deaths also normalized.
  - **Ebola scare in the U.S.**
    - Only two people were infected on U.S. soil (nurses treating a patient).
    - Transmission requires direct contact with bodily fluids and symptomatic individuals.
    - Despite extremely low actual risk (effectively 0 for 99.99% of Americans), media coverage and emotional salience produced nationwide panic.  
    → Availability + strong affect (fear) distort risk perception.

- **Pitfalls / Common Confusions**
  - **Confusing ease of recall with true probability**
    - Just because an event is easy to remember (e.g., terrorism, shark attacks, dog maulings) does not mean it is objectively likely.
  - **Not noticing question substitution**
    - Believing you’re answering “How dangerous is this really?” when you’re actually answering “How much does this stick in my mind?”
  - **Moral/value judgment about System 1 vs System 2**
    - Terms like “lizard brain” vs “neo-cortex” encourage seeing one type of thinking as inferior; Kahneman’s “system 1 / system 2” are neutral on value.
    - Both systems are necessary: System 1 is crucial for survival (fast responses); System 2 for complex reasoning.
  - **Over-trusting intuition for complex policy questions**
    - Heuristics are useful, but for complex, high-stakes policy (e.g., Breed-Specific Legislation, risk regulation), relying on availability and affect without System 2 analysis can yield systematically wrong decisions.
